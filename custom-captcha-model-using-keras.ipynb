{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "* This example demonstrates a simple OCR model built with the Functional API. Apart from combining CNN and RNN, it also illustrates how you can instantiate a new layer and use it as an \"Endpoint layer\" for implementing CTC loss. For a detailed guide to layer subclassing, please check out this page in the developer guides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import ops\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data: Captcha Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The dataset contains 400 captcha files as png images. The label for each sample is a string, the name of the file (minus the file extension). We will map each character in the string to an integer for training the model. Similary, we will need to map the predictions of the model back to strings. For this purpose we will maintain two dictionaries, mapping characters to integers, and integers to characters, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir=Path(\"./clean_captchas\")\n",
    "\n",
    "images = sorted(list(map(str, list(data_dir.glob(\"*.png\")))))\n",
    "labels = [img.split(os.path.sep)[-1].split(\".png\")[0] for img in images]\n",
    "characters = set(char for label in labels for char in label)\n",
    "characters = sorted(list(characters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of images found: \", len(images))\n",
    "print(\"Number of labels found: \", len(labels))\n",
    "print(\"Number of unique characters: \", len(characters))\n",
    "print(\"Characters present: \", characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=16\n",
    "image_width=200\n",
    "image_height=50\n",
    "\n",
    "downsample_factor=4 # This reflects two MaxPooling2D((2,2)) layers\n",
    "max_length=max([len(label) for label in labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping characters to integers\n",
    "char_to_num = layers.StringLookup(vocabulary=list(characters), mask_token=None, num_oov_indices=0, dtype=\"int32\")\n",
    "\n",
    "# Mapping integers back to original characters\n",
    "num_to_char = layers.StringLookup(vocabulary=char_to_num.get_vocabulary(), mask_token=None, num_oov_indices=0, invert=True, dtype=\"int32\")\n",
    "\n",
    "\n",
    "def split_data(images_paths_list, labels_list, train_size=0.9, shuffle=True):\n",
    "    size = len(images_paths_list)\n",
    "    indices = ops.arange(size)\n",
    "    if shuffle:\n",
    "        indices = keras.random.shuffle(indices, seed=42)\n",
    "    train_samples = int(size * train_size)\n",
    "    x_train_paths_arr, y_train_labels_arr = images_paths_list[indices[:train_samples]], labels_list[indices[:train_samples]]\n",
    "    x_valid_paths_arr, y_valid_labels_arr = images_paths_list[indices[train_samples:]], labels_list[indices[train_samples:]]\n",
    "    return x_train_paths_arr, x_valid_paths_arr, y_train_labels_arr, y_valid_labels_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting data into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_paths_np=np.array(images)\n",
    "train_labels_np=np.array(labels)\n",
    "\n",
    "x_train_paths, x_valid_paths, y_train_labels, y_valid_labels = split_data(train_image_paths_np,train_labels_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_single_sample(img_path, label):\n",
    "    img = tf.io.read_file(img_path)\n",
    "    img = tf.io.decode_png(img, channels=1)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    img = tf.where(img < 0.5, 0.0, 1.0)\n",
    "    img = ops.image.resize(img, [image_height, image_width])\n",
    "    label_tensor = char_to_num(tf.strings.unicode_split(label, input_encoding=\"UTF-8\"))\n",
    "    return {\"image\": img, \"label\": label_tensor}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train_paths, y_train_labels))\n",
    "train_dataset = (train_dataset.map(encode_single_sample, num_parallel_calls=tf.data.AUTOTUNE).batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE))\n",
    "\n",
    "validation_dataset = tf.data.Dataset.from_tensor_slices((x_valid_paths, y_valid_labels))\n",
    "validation_dataset = (validation_dataset.map(encode_single_sample, num_parallel_calls=tf.data.AUTOTUNE).batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(nrows=4, ncols=4, figsize=(15, 10))\n",
    "for batch in train_dataset.take(1):\n",
    "    images_viz = batch[\"image\"]\n",
    "    labels_viz = batch[\"label\"]\n",
    "    for i in range(16):\n",
    "        img_viz = (images_viz[i] * 255).numpy().astype(\"uint8\")\n",
    "        label_viz = tf.strings.reduce_join(num_to_char(labels_viz[i])).numpy().decode(\"utf-8\")\n",
    "        ax[i // 4, i % 4].imshow(img_viz[:, :, 0], cmap=\"gray\")\n",
    "        ax[i // 4, i % 4].set_title(label_viz)\n",
    "        ax[i // 4, i % 4].axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctc_label_dense_to_sparse(labels, label_lengths):\n",
    "    label_shape = ops.shape(labels)\n",
    "    num_batches_tns = ops.stack([label_shape[0]])\n",
    "    max_num_labels_tns = ops.stack([label_shape[1]])\n",
    "\n",
    "    def range_less_than(old_input, current_input):\n",
    "        return ops.expand_dims(ops.arange(ops.shape(old_input)[1]), 0) < tf.fill(max_num_labels_tns, current_input)\n",
    "\n",
    "    init = ops.cast(tf.fill([1, label_shape[1]], 0), dtype=\"bool\")\n",
    "    dense_mask = tf.compat.v1.scan(range_less_than, label_lengths, initializer=init, parallel_iterations=1)\n",
    "    dense_mask = dense_mask[:, 0, :]\n",
    "\n",
    "    label_array = ops.reshape(ops.tile(ops.arange(0, label_shape[1]), num_batches_tns), label_shape)\n",
    "    label_ind = tf.compat.v1.boolean_mask(label_array, dense_mask)\n",
    "\n",
    "    batch_array = ops.transpose(ops.reshape(ops.tile(ops.arange(0, label_shape[0]), max_num_labels_tns),tf.reverse(label_shape, [0])))\n",
    "    batch_ind = tf.compat.v1.boolean_mask(batch_array, dense_mask)\n",
    "    indices = ops.transpose(ops.reshape(ops.concatenate([batch_ind, label_ind], axis=0), [2, -1]))\n",
    "\n",
    "    vals_sparse = tf.compat.v1.gather_nd(labels, indices)\n",
    "    return tf.SparseTensor(ops.cast(indices, dtype=\"int64\"), ops.cast(vals_sparse, dtype=\"int32\"), ops.cast(label_shape, dtype=\"int64\"))\n",
    "\n",
    "def ctc_batch_cost(y_true, y_pred, input_length, label_length):\n",
    "    label_length_squeezed = ops.cast(ops.squeeze(label_length, axis=-1), dtype=\"int32\")\n",
    "    input_length_squeezed = ops.cast(ops.squeeze(input_length, axis=-1), dtype=\"int32\")\n",
    "    \n",
    "    sparse_labels = ctc_label_dense_to_sparse(y_true, label_length_squeezed)\n",
    "\n",
    "    y_pred_transposed = ops.transpose(y_pred, axes=[1, 0, 2])\n",
    "    \n",
    "    loss = tf.compat.v1.nn.ctc_loss(\n",
    "        labels=sparse_labels, \n",
    "        inputs=y_pred_transposed, \n",
    "        sequence_length=input_length_squeezed,\n",
    "        preprocess_collapse_repeated=False,\n",
    "        ctc_merge_repeated=True,\n",
    "        ignore_longer_outputs_than_inputs=False\n",
    "    )\n",
    "    return ops.expand_dims(loss, 1)\n",
    "\n",
    "class CTCLayer(layers.Layer):\n",
    "    def __init__(self, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.loss_fn = ctc_batch_cost\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        batch_len = ops.cast(ops.shape(y_true)[0], dtype=\"int64\")\n",
    "        input_length = ops.cast(ops.shape(y_pred)[1], dtype=\"int64\")\n",
    "        label_length = ops.cast(tf.fill((batch_len, 1), max_length), dtype=\"int64\")\n",
    "        input_length_expanded = input_length * ops.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "\n",
    "        loss = self.loss_fn(y_true, y_pred, input_length_expanded, label_length)\n",
    "        self.add_loss(loss)\n",
    "        return y_pred\n",
    "\n",
    "def build_model():\n",
    "    input_img = layers.Input(shape=(image_height, image_width, 1), name=\"image\", dtype=\"float32\")\n",
    "    labels = layers.Input(name=\"label\", shape=(None,), dtype=\"int32\")\n",
    "\n",
    "    x = layers.Conv2D(32,(3, 3),activation=\"relu\",kernel_initializer=\"he_normal\",padding=\"same\",name=\"Conv1\")(input_img)\n",
    "    x = layers.MaxPooling2D((2, 2), name=\"pool1\")(x) \n",
    "\n",
    "    x = layers.Conv2D(64,(3, 3),activation=\"relu\",kernel_initializer=\"he_normal\",padding=\"same\",name=\"Conv2\")(x)\n",
    "    x = layers.MaxPooling2D((2, 2), name=\"pool2\")(x) \n",
    "\n",
    "    x = layers.Permute((2, 1, 3))(x)  \n",
    "    new_shape = ((image_width // downsample_factor), (image_height // downsample_factor) * 64)\n",
    "    x = layers.Reshape(target_shape=new_shape, name=\"reshape\")(x) \n",
    "    \n",
    "    x = layers.Dense(64, activation=\"relu\", name=\"dense1\")(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "\n",
    "    x = layers.Bidirectional(layers.LSTM(128, return_sequences=True, dropout=0.25))(x)\n",
    "    x = layers.Bidirectional(layers.LSTM(64, return_sequences=True, dropout=0.25))(x)\n",
    "\n",
    "    num_classes_for_ctc = len(characters) + 1 \n",
    "    x = layers.Dense(num_classes_for_ctc, activation=\"softmax\", name=\"dense2\")(x)\n",
    "\n",
    "    output = CTCLayer(name=\"ctc_loss\")(labels, x)\n",
    "\n",
    "    model = keras.models.Model(inputs=[input_img, labels], outputs=output, name=\"ocr_model_v1\")\n",
    "    opt = keras.optimizers.Adam()\n",
    "    model.compile(optimizer=opt)\n",
    "    return model, input_img # Return input_img as well\n",
    "\n",
    "# Get the model\n",
    "model, input_img_tensor = build_model()\n",
    "model.summary(expand_nested=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100 \n",
    "early_stopping_patience = 15 \n",
    "\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=early_stopping_patience, restore_best_weights=True)\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6, verbose=1)\n",
    "\n",
    "history = model.fit(train_dataset,validation_data=validation_dataset,epochs=epochs,callbacks=[early_stopping, reduce_lr])"
   ]
  },
    {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['loss'], label='Training Loss', marker='o')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss', marker='o')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctc_decode(y_pred, input_length, greedy=True, beam_width=100, top_paths=1):\n",
    "    input_shape = ops.shape(y_pred)\n",
    "    num_samples, num_steps = input_shape[0], input_shape[1]\n",
    "    y_pred_transposed = ops.transpose(y_pred, axes=[1, 0, 2])\n",
    "    input_length_squeezed = ops.cast(ops.squeeze(input_length, axis=-1), dtype=\"int32\")\n",
    "\n",
    "    if greedy:\n",
    "        (decoded, log_prob) = tf.nn.ctc_greedy_decoder(inputs=y_pred_transposed, sequence_length=input_length_squeezed)\n",
    "    else:\n",
    "        (decoded, log_prob) = tf.compat.v1.nn.ctc_beam_search_decoder(inputs=y_pred_transposed, sequence_length=input_length_squeezed, beam_width=beam_width, top_paths=top_paths)\n",
    "    \n",
    "    decoded_dense = []\n",
    "    for st in decoded:\n",
    "        st = tf.SparseTensor(st.indices, st.values, (num_samples, num_steps))\n",
    "        decoded_dense.append(tf.sparse.to_dense(sp_input=st, default_value=-1))\n",
    "    return (decoded_dense, log_prob)\n",
    "\n",
    "prediction_model = keras.models.Model(input_img_tensor, model.get_layer(name=\"dense2\").output)\n",
    "\n",
    "def decode_batch_predictions(pred):\n",
    "    input_len = np.ones((pred.shape[0], 1)) * pred.shape[1] \n",
    "    results = ctc_decode(pred, input_length=input_len, greedy=True)[0][0][:, :max_length]\n",
    "    output_text = []\n",
    "    for res_tensor in results:\n",
    "        filtered_indices = tf.gather_nd(res_tensor, tf.where(tf.logical_and(res_tensor != -1, res_tensor < len(characters))))\n",
    "        res_text = tf.strings.reduce_join(num_to_char(filtered_indices)).numpy().decode(\"utf-8\")\n",
    "        output_text.append(res_text)\n",
    "    return output_text\n",
    "\n",
    "for batch in validation_dataset.take(1):\n",
    "    batch_images = batch[\"image\"]\n",
    "    batch_labels = batch[\"label\"]\n",
    "\n",
    "    preds = prediction_model.predict(batch_images)\n",
    "    pred_texts = decode_batch_predictions(preds)\n",
    "\n",
    "    orig_texts = []\n",
    "    for label_tensor in batch_labels:\n",
    "        filtered_label = tf.gather_nd(label_tensor, tf.where(label_tensor < len(characters))) \n",
    "        label_text = tf.strings.reduce_join(num_to_char(filtered_label)).numpy().decode(\"utf-8\")\n",
    "        orig_texts.append(label_text)\n",
    "\n",
    "    _, ax = plt.subplots(4, 4, figsize=(15, 5))\n",
    "    for i in range(min(len(pred_texts), 16)):\n",
    "        img_viz = (batch_images[i, :, :, 0] * 255).numpy().astype(np.uint8)\n",
    "        title = f\"Orig: {orig_texts[i]}\\nPred: {pred_texts[i]}\"\n",
    "        ax[i // 4, i % 4].imshow(img_viz, cmap=\"gray\")\n",
    "        ax[i // 4, i % 4].set_title(title)\n",
    "        ax[i // 4, i % 4].axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
